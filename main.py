from fastapi import FastAPI, UploadFile, File, Form
from pydantic import BaseModel
from fastapi.responses import JSONResponse
from typing import List, Optional, Union
from openai import OpenAI
from dotenv import load_dotenv
import json
from utils.time.image_parser import get_schedule_mask
from utils.time.visualize import visualize_free_mask
from utils.grad.analyzer import analyze_graduation_pdf
import numpy as np

# í™˜ê²½ ë³€ìˆ˜ ë¡œë”©
load_dotenv()
client = OpenAI()

# ê³¼ëª© ë°ì´í„° ë¡œë”©
with open("data/cse_course.json", "r", encoding="utf-8") as f:
    curriculum = json.load(f)

# FastAPI ì•± ì´ˆê¸°í™”
app = FastAPI()

# ìš”ì²­ ëª¨ë¸
class RecommendRequest(BaseModel):
    keyword: str
    add_info: str

# ì¶”ì²œ í•­ëª© ëª¨ë¸
class RecommendationItem(BaseModel):
    title: str
    description: str

# ì‘ë‹µ ëª¨ë¸
class RecommendResponse(BaseModel):
    keyword: str
    add_info: str
    ai_response: str  # ë¬¸ìì—´ ì‘ë‹µìœ¼ë¡œ êµ¬ì„±

# ì—ëŸ¬ ì‘ë‹µ ëª¨ë¸
class ErrorResponse(BaseModel):
    error: str
    raw_text: Optional[str] = None
    warning: Optional[str] = None

class ChatRequest(BaseModel):
    question: str

class ChatResponse(BaseModel):
    question: str
    ai_add_response: str

class DeficiencyItem(BaseModel):
    í•­ëª©: str
    ë¶€ì¡±_ë‚´ìš©: str
    ë³´ì™„_ë°©ë²•: str

class GraduationResult(BaseModel):
    ì¡¸ì—…_ê°€ëŠ¥: bool
    ë¶€ì¡±_í•­ëª©: Optional[List[DeficiencyItem]]
    ì¢…í•©_ì˜ê²¬: str
    ê²€ì¦_ë¶€ì¡±í•­ëª©: Optional[List[str]] = None
##### ê³¼ëª© ì¶”ì²œ #####
@app.post("/recommend", response_model=Union[RecommendResponse, ErrorResponse])
async def recommend(request: RecommendRequest):
    keyword = request.keyword.lower()

    # ê´€ë ¨ í‚¤ì›Œë“œ ê³¼ëª© í•„í„°ë§
    related = [c for c in curriculum if keyword in [k.lower() for k in c.get("keywords", [])]]
    samples = related[:5] if related else curriculum[:5]

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    context_text = "\n".join([
        f"- {c['name']} ({c['category']}, {c['credit']}í•™ì ): {c.get('description', 'ì„¤ëª… ì—†ìŒ')}"
        for c in samples
    ])

    prompt = f"""
    ë„ˆëŠ” ëŒ€í•™ìƒì—ê²Œ ê³¼ëª©ì„ ì¶”ì²œí•´ì£¼ëŠ” AI ì¡°êµì•¼.
    ì•„ë˜ ì‚¬ìš©ì ì •ë³´ì™€ ê³¼ëª© ëª©ë¡ì„ ì°¸ê³ í•´ì„œ 3ê°œ ê³¼ëª©ì„ ì¶”ì²œí•´ì¤˜.
    ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´:

    {{
    "recommendations": [
        {{
        "title": "ê³¼ëª©ëª…",
        "description": "ì„¤ëª…"
        }},
        ...
    ]
    }}

ğŸ” ê´€ì‹¬ ë¶„ì•¼: {request.keyword}
â„¹ï¸ ì¶”ê°€ ì •ë³´: {request.add_info}
ê³¼ëª© ëª©ë¡:
{context_text}
    """

    try:
        # OpenAI API í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë„ˆëŠ” AI ì¡°êµì•¼. ë°˜ë“œì‹œ JSONë§Œìœ¼ë¡œ ë‹µë³€í•´."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.4
        )

        result_raw = response.choices[0].message.content.strip()
        print("ğŸ§  AI ì‘ë‹µ ì›ë¬¸:\n", result_raw)

        result_json = json.loads(result_raw)
        recommendations = result_json.get("recommendations", [])

        # ë¬¸ìì—´ë¡œ ì‘ë‹µ í¬ë§·íŒ…
        formatted_response = "\n".join([
            f"{i+1}. {r['title']} - {r['description']}" for i, r in enumerate(recommendations)
        ])

        return RecommendResponse(
            keyword=request.keyword,
            add_info=request.add_info,
            ai_response=formatted_response
        )

    except json.JSONDecodeError:
        return ErrorResponse(
            error="JSON íŒŒì‹± ì‹¤íŒ¨",
            raw_text=result_raw,
            warning="AI ì‘ë‹µì„ JSONìœ¼ë¡œ ë³€í™˜í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        )

    except Exception as e:
        return ErrorResponse(error=str(e))

##### ì§ˆì˜ì‘ë‹µ #####
conversation_history = []
@app.post("/chat", response_model=ChatResponse)
async def chat(req: ChatRequest):
    question = req.question.strip()

    # ê³¼ëª© ë°ì´í„° ìš”ì•½ (ìµœëŒ€ 100ê°œë§Œ)
    summarized = "\n".join([
        f"[{c['code'] or 'N/A'}] {c['name']} | {c['category']}, {c['credit']}í•™ì , {c['year']}í•™ë…„ | í‚¤ì›Œë“œ: {', '.join(c.get('keywords', []))}\nì„¤ëª…: {c.get('description', 'ì„¤ëª… ì—†ìŒ')}"
        for c in curriculum[:1000]
    ])

    # AI ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = """
    ë„ˆëŠ” ê²½í¬ëŒ€ ì»´í“¨í„°ê³µí•™ê³¼ GPT ì¡°êµì•¼.
    í•™ìƒì´ ì»¤ë¦¬í˜ëŸ¼ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•˜ë©´ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•´ì¤˜.
    ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì •ë³´ëŠ” "ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë§í•´.
    """

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì§ˆë¬¸ ì¶”ê°€
    conversation_history.append({"role": "user", "content": question})

    try:
        # OpenAI API í˜¸ì¶œ
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": summarized}
            ] + conversation_history,
            temperature=0.4
        )

        reply = response.choices[0].message.content.strip()
        conversation_history.append({"role": "assistant", "content": reply})

        # ChatResponse í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
        return ChatResponse(
            question=question,
            ai_add_response=reply
        )

    except Exception as e:
        return JSONResponse(
            content={"error": str(e)},
            status_code=500
        )


from fastapi import UploadFile, File, Query
from fastapi.responses import JSONResponse

@app.post("/analyze-pdf")
async def analyze_pdf(
    major: str = Query(..., description="í•™ê³¼"),
    student_id: str = Query(..., description="í•™ë²ˆ"),
    file: UploadFile = File(...)
):
    result = await analyze_graduation_pdf(file, major, student_id)
    return {"result": result}


##### ê³µê°• ì‹œê°„í‘œ #####
@app.post("/timetable")
async def analyze_schedule_image(files: list[UploadFile]):
    masks = []

    for file in files:
        save_path = f"AI/data/ì‹œê°„í‘œ/{file.filename}"
        with open(save_path, "wb") as f:
            f.write(await file.read())

        mask = get_schedule_mask(save_path)
        masks.append(mask)

    # ê³µê°• ë§ˆìŠ¤í¬ ê³„ì‚° (ëª¨ë‘ ìˆ˜ì—… ì—†ì„ ë•Œë§Œ ê³µê°•)
    combined = np.stack(masks, axis=0)
    free_mask = np.all(combined == 0, axis=0)

    result_path = "AI/data/ê³µí†µê³µê°•22.png"
    visualize_free_mask(free_mask, result_path)

    return {
        "message": "ê³µê°• ì‹œê°í™” ì™„ë£Œ",
        "image": result_path
    }